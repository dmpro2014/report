\documentclass[../main/report.tex]{subfiles}
\begin{document}

\chapter{GPU}

We know how to write a kernel, and how to start it.
Let's deep dive into the heart of Demolicious; the GPU!
In this chapter we follow our kernel all the way from the GPU gets the commands from the CPU,
through the execution of all the threads, into memory and finally to the screen over HDMI.

The Demolicious GPU is implemented on a Spartan-6 FPGA, a programmable hardware chip.
The architecture has been designed, sketches drawn, and lastly implemented with VHDL;
a hardware definition language.

\section{Responsibilities}

The GPU has the following responsibilities:
\begin{enumerate}
  \item
    Receive instructions and constants from the CPU
  \item
    Handle kernel invocations from the CPU
  \item
    Write results to external SRAM
  \item
    Assert the 'computation finished' signal to the CPU
\end{enumerate}

\section{Architecture Overview}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{../gpu/diagrams/architecture_overview.png}
\caption{A high level overview of the GPU.}
\label{fig:architecture_overview}
\end{figure}

Figure \ref{fig:architecture_overview} presents a high level overview over the GPU.
The CPU issues commands to the communication unit in the GPU. Commands are launching a kernel, uploading kernels to the instruction memory, writing to the constant memory, and read/write to SRAM.
Instructions are fetched from the instruction memory and decoded by the control unit, which has the responsibility of setting the control signals for the instructions.
The processors load constants from the constant memory, and uses the load/store unit for accessing the data memory.

\emph{Insert stats on how many cores etc etc. 1 SM@@@}


\section{Receiving a Kernel Call}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{../gpu/diagrams/receiving_a_kernel_call.png}
\caption{Launching a kernel from the GPU's viewpoint.}
\label{fig:kernel_call}
\end{figure}

The communication unit is responsible for receiving kernel call requests from the CPU.
When a kernel call is received, the kernel launch signal is asserted.
A kernel call consists of the address of the kernel, and the number of threads to launch.

The kernel launch signals are forwarded to the thread spawner, which writes the kernel start address to the PC register, and starts distributing thread IDs to the processor cores. 
After holding the kernel launch signals high, the communication unit has completed its role in launching the kernel.
When the kernel completes executing, the thread spawner asserts the kernel done signal, and the communication unit forwards the signal to the CPU, indicating that the kernel call has completed.

\section{Running a Kernel}

\subsection{Warps}

As in the NVIDIA GPU architectures, see chapter \ref{sec:fermi}, Demolicious groups threads into \emph{warps}.
These warps contain 8 threads, one per core. 
For Demolicious versions with fewer cores, the warp size changes accordingly.
Just as the warps in the NVIDIA architecture, every thread in these warps always execute the same instruction at the same time, one in each core.
The main difference is that while NVIDIA has multiple \emph{Streaming Multiprocessors} which can each execute a warp at the same time, we can only execute a single warp at the time.
In a way, the Demolicious system is analogous to having a single \emph{Streaming Multiprocessor}.

Another central difference in the Demolicious warps is that since our kernels have no jumps, the threads never diverge.
This significantly reduces the complexity of scheduling these warps, and let's us get away with a single program counter.

\subsection{Static scheduling}

\emph{Many mem requests per clock@}
As every thread in a warp always executes the same instruction, our system has to handle 8 memory requests being issued during the same cycle.
The memory system, consisting of two SRAM chips, can return two words every cycle, with a single cycle delay.
This is not sufficient bandwidth to satisfy the memory requests without stalling.
Some of the stalling can be hidden if the programmer manually loads the memory word enough instructions before usage, but even with this strategy there will be multiple programs that simply doesn't have enough indepentent instructions to execute while waiting for memory, and have to insert \emph{noops}, effectively stalling the GPU.

But there are many warps waiting for execution at (almost) any time. 
If these warps could execute while other warps are waiting for memory, we can utilize the system better.
However, changing between warps that execute requires a context switch. 
That is, the old thread put on hold has to store all its register values somewhere so that they can be available when it starts executing again.
\todo{cite something?}
In software threading, a context switch is typically achieved by store all registers to memory and loading in the registers for the new thread that is scheduled.
This is an expensive operation, and would introduce more latency than we are trying to hide.
Demolicious, however, has a set of active warps that all have their own registers.
So a context switch can be carried out with virtually no overhead, at the expensive of some additional hardware to store all these extra registers.

To keep the architecture as simple, Demolicious employs a simple static scheduling algorithm. 
The active warps are simply rotated every instruction.
\todo{insert figure and example walkthrough}

However, there is an issue with this simple scheduling algorithm.  
 



\emph{Hide this with barrel}




\emph{Bundling the writes. This bad. @}

\emph{Solve by stagger@}

\emph{Walk through example@}


\subsection{Should this section be moved? What to call @}

Once the thread spawner has been initiated by the communication unit, the kernel runs to completion without intervention by the CPU. 
When a kernel run starts, the thread spawner assigns thread IDs to each warp in the barrel.
After the initial IDs have been assigned the GPU enters normal execution.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{../gpu/diagrams/kernel_run_state_machine.png}
    \caption{The GPU's internal state during kernel execution.}
    \label{fig:kernel_run_state_machine}
\end{figure}
A normal kernel execution can be represented by the state machine in figure \ref{fig:kernel_run_state_machine}.
During the kernel execution stage the threads in a warp execute the same instructions, until the control unit encounters a \emph{finished} instruction.
Upon receiving a \emph{finished} instruction the control unit asserts the \emph{finished} signal alerting the thread spawner that a new warp has to be spawned.
The thread spawner keeps track of the number of threads awaiting launch.
When the thread spawner receives a \emph{finished} instruction and no more warps are awaiting launch, the kernel complete signal is asserted, and the kernel run has completed.


\section{Module Details}

\subfile{../gpu/core.tex}

\subfile{../gpu/thread-spawner.tex}

\subfile{../gpu/regdir.tex}

\subfile{../gpu/sram.tex}

\subfile{../gpu/lsu.tex}

\subfile{../gpu/hdmi.tex}

\subsection{Summary}
The journey of our kernel is complete.
We have followed it all the way from the initial \emph{load\_kernel} call to the screen.
It has traveled through the massively parallel GPU which can handle a vast amount of threads,
by using hardware threads with a round-robin static scheduling technique called barrel processing.

\end{document}
