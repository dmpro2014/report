\documentclass[../main/report.tex]{subfiles}
\begin{document}

\chapter{GPU}

We know how to write a kernel, and how to start it.
Let's deep dive into the heart of Demolicious; the GPU!
In this chapter we follow our kernel all the way from the GPU gets the commands from the CPU,
through the execution of all the threads, into memory and finally to the screen over HDMI.

The Demolicious GPU is implemented on a Spartan-6 FPGA, a programmable hardware chip.
The architecture has been designed, sketches drawn, and lastly implemented with VHDL;
a hardware definition language.

\section{Responsibilities}

The GPU has the following responsibilities:
\begin{enumerate}
  \item
    Receive instructions and constants from the CPU
  \item
    Handle kernel invocations from the CPU
  \item
    Write results to external SRAM
  \item
    Assert the 'computation finished' signal to the CPU
\end{enumerate}

\section{Architecture Overview}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{../gpu/diagrams/architecture_overview.png}
\caption{A high level overview of the GPU.}
\label{fig:architecture_overview}
\end{figure}

Figure \ref{fig:architecture_overview} presents a high level overview over the GPU.
The CPU issues commands to the communication unit in the GPU.
Instructions are fetched from the instruction memory and decoded by the control unit,
which has the responsibility of setting the control signals for the instructions.
Control signals go to all cores of the GPU.
The processor cores access memory through the load/store unit, and get constants from the constant memory.
The video unit reads pixels from the data memory and outputs them to a screen over HDMI.

\section{Receiving a Kernel Call}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{../gpu/diagrams/receiving_a_kernel_call.png}
\caption{Launching a kernel from the GPU's viewpoint.}
\label{fig:kernel_call}
\end{figure}

The communication unit is responsible for receiving kernel call requests from the CPU.
When a kernel call is received, the kernel launch signal is asserted.
A kernel call consists of the address of the kernel, and the number of threads to launch.

The kernel launch signals are forwarded to the thread spawner, which writes the kernel start address to the PC register, and starts distributing thread IDs to the processor cores. 
After holding the kernel launch signals high, the communication unit has completed its role in launching the kernel.
When the kernel completes executing, the thread spawner asserts the kernel done signal, and the communication unit forwards the signal to the CPU, indicating that the kernel call has completed.

\section{Running a Kernel}

\subsection{Warps}
\label{subsec:warps}

As in NVIDIA's Fermi architecture (see chapter \ref{sec:fermi}), Demolicious organizes threads into \emph{warps}.
In Demolicious, the warp size is the same as the number of processor cores.
For versions with fewer cores, the warp size changes accordingly.
Just as the warps in Fermi, every thread in these warps always execute the same instruction at the same time, with one thread in each core.
Note that while Fermi has multiple \emph{Streaming Multiprocessors} which can each execute warps concurrently, Demolicious can only execute a single warp at a time.
The Demolicious system is analogous to having a single Fermi \emph{Streaming Multiprocessor}.

An important difference between Demolicious and Fermi, is that Demolicious kernels have no jumps, so its threads never diverge.
This significantly reduces the complexity of scheduling these warps, and let's us get away with a single program counter.

\subsection{Static Scheduling}

% Many mem requests per clock
As every thread in a warp always executes the same instruction, our system needs to handle 8 memory requests being issued during the same cycle.
The memory system for Demolicious consists of two SRAM chips, and can return two words every cycle.
%A memory request has a single cycle delay.
This bandwidth is not sufficient to satisfy the set of 8 memory requests produced by a warp without stalling.
Occasionally, these stalls can be hidden by the programmer if loads are issued ahead of their usage.
However, when the program does not have enough useful instructions to do while waiting for loads, the GPU must stall.
Stalling a throughput-oriented machine is obviously unfortunate.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.25]{../gpu/diagrams/uten_jaktstart_num.png}
\caption{Execution timing of warps without jagged scheduling}
\label{fig:nojagged}
\end{figure}

But there are many warps waiting for execution at (almost) any time.
If these warps could execute while other warps are waiting for memory, we can utilize the system better.
However, changing between warps that execute requires a context switch.
That is, the old thread put on hold has to store all its register values somewhere so that they can be available when it starts executing again.
In software multi-threading, a context switch is typically achieved by storing all registers to memory and loading in the registers for the new thread that is scheduled.
This is an expensive operation, and would introduce more latency than we are trying to hide.
Demolicious, however, has a set of active warps that all have their own registers.
So a context switch can be carried out with virtually no overhead, at the expense of some additional hardware to store all these extra registers.

To keep the architecture as simple as possible, Demolicious employs a simple static scheduling algorithm.
The active warps are simply rotated after every instruction. In figure \ref{fig:nojagged} we revisit our green-screen kernel, and see how the warps are scheduled.
The numbers in the top right corner is the gpu cycle, and we refer to the rows as barrel lines.
During cycle 0, instruction A is executed on warp 0.
The next cycle, instruction A is executed on warp 1.
And when we get to cycle 4, warp 0 is scheduled again, this time executing instruction B.
This continues until we reach clock cycle 16-19 where warps 0 through 3 executes instruction E, the \emph{thread\_finished} instruction, which does no computation, but allows the GPU to set up new warps.
On cycle 20, warp 4 is set up and ready to execute the first instruction.
This pattern continues until all threads have run to completion.

However, there is an issue with this simple scheduling algorithm.
Let's see what happens when we get to instruction D, marked in red.
In clock cycle 12\footnote{The cycle numbers are written in the upper right corner of each square} all threads in warp 0 execute a memory request, resulting in 8 simultaneous memory requests.
As our memory system handles only two requests per clock,
it is busy the next 4 cycles with these requests.
When warp 0 gets to execute its next instruction, the memory operation is complete for all the threads in the warp.
Had this been a load instead of a store, the loaded value could already be used its instruction, and we have avoided a stall.
So far, so good.
However, when warp 1 executes the store instruction in cycle 13, the memory is already busy with the request from warp 0.
The memory requests from warp 1 will not be started before cycle 16, and will therefore not be complete for its next instruction.
It is even worse for warp 2 and 3. As all warps execute the same code, this can introduce up to 8 \emph{nop} instructions for every warp.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.25]{../gpu/diagrams/jaktstart_num.png}
\caption{Execution timing of warps with jagged scheduling}
\label{fig:jagged}
\end{figure}

Clearly, the introduction of this barrel processing technique did not solve our problems.
Fortunately, this was a simplified version of the scheduling in Demolicious.
The actual scheduling is shown in figure \ref{fig:jagged}.
By introducing an offset in the instructions executed, henceforth called jagged execution\footnote{Or "jaktstart" as we like to call it in Norwegian}, we can avoid this issue.

With this scheduling, you can see that there are 4 cycles without a memory operation between every memory operation.
This is true as long as memory operations have 4 non-memory operations between them in the kernel.
It is up to the programmer or assembler to ensure that this holds.

This warp scheduling algorithm will guarantee that nops are not required after loads,
as long as they are spaced far enough apart.

\section{Module Details}

\subfile{../gpu/core.tex}

\subfile{../gpu/thread-spawner.tex}

\subfile{../gpu/regdir.tex}

\subfile{../gpu/sram.tex}

\subfile{../gpu/lsu.tex}

\subfile{../gpu/hdmi.tex}

\subsection{Summary}
The journey of our kernel is complete.
We have followed it all the way from the initial \emph{load\_kernel} call to the screen.
It has traveled through the massively parallel GPU, which can handle a vast amount of threads,
using a round-robin static scheduling technique called barrel processing.

\end{document}
