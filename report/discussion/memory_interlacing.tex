\section{SRAM - To Interlace or not to Interlace}

The presented architecture uses interlaced SRAM banks, presenting one large unified memory space.
An alternative approach is to dedicate one SRAM bank to the LSU, the other to the HDMI unit.
With dedicated banks there will be no need for arbitration, as there never will be more than one unit accessing each at the time.
This would result in the HDMI unit never starving due to memory lockout from the GPU.
The LSU would also be simpler, needing only a single queue to service memory requests.

The major advantage of interlacing the address space of the two memory banks is that memory throughput effectively doubles.
Two requests can be processed in parallel from the unit currently granted access by the SRAM arbiter.
This double data rate allows for a smaller design footprint, reducing the number of hardware resources required.
When using dedicated SRAM banks, the GPU barrel height has to be equal to the number of execution cores.
This due to that when a warp of size $n$ requests memory access, it will take at least $n$ cycles to dispatch all requests, as there is only one SRAM bank to service them.
With the interlaced memory however, two memory requests can be serviced each cycle, given that the memory requests are balanced equally across SRAM banks (this is enforced at the hardware level).
This allows all register directories to shrink in size from $n$ to $n/2$.

As the register directories compose a significant part of the design footprint, the barrel height reduction is significant.
Signal fanout is also reduced from- and to the register banks, dampening the reduction in clock frequency when scaling the number of computation cores.
More hardware resources can now be allocated to processor cores, power is saved due to less registers being used, and the clock frequency can stay higher.
Drawbacks include the occasional flicker due to HDMI unit starvation.
